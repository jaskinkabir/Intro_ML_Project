\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in
% the first footnote. If that is unneeded, please comment it
% out.
\usepackage{cite}
\usepackage{hyperref}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{placeins}
\usepackage{titlesec}

\setcounter{secnumdepth}{4}



\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em
    b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{ Fire Detection With Erroneous Input Correction\\
        { \footnotesize ECGR 4105/5105 - Introduction to
        Machine Learning} }

\author{\IEEEauthorblockN{1\textsuperscript{st} Jaskin
Kabir} \IEEEauthorblockA{\textit{ECGR Student} \\
\textit{UNC Charlotte}\\
Charlotte, NC \\
jkabir@charlotte.edu}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Axel Leon Vasquez}
\IEEEauthorblockA{\textit{ECGR Student} \\
\textit{UNC Charlotte}\\
Charlotte, NC \\
aleonvas@charlotte.edu}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Matthew Anderson}
\IEEEauthorblockA{\textit{ECGR Student} \\
\textit{UNC Charlotte}\\
Charlotte, NC \\
mande137@charlotte.edu} }
\maketitle

\begin{abstract}
The accuracy of typical fire alarm systems is often limited
due to their reliance on single sensors with constrained
detection capabilities. Moreover, a sensor failure renders
the entire system ineffective, posing a critical risk. While
AI-based fire detectors have massively outperformed
traditional systems, little work has addressed handling
sensor failures. To address this challenge, we propose
\textbf{AlarmNet}, a robust fire detection system that
incorporates erroneous sensor data during training. We
investigate two implementations of AlarmNet: a centralized
model trained on a single server and a simulation of a
federated model where training is distributed across
multiple clients and aggregated into a global model. In our
experiments, sensor errors were introduced by randomly
replacing 24.2\% of the datasetâ€™s measurements with missing
or faulty values. We applied a pre-processing step that
replaced these values with the median of the corresponding
training features. Our 3-layer artificial neural network The
model achieved an accuracy of 98.38\% and successfully
detected 99.73\% of fires, despite the high rate of sensor
errors. This resulted in only a 0.25\% reduction in
detection performance compared to a model trained on
complete data, and a significant 41.51\% improvement over a
simulated commercial-grade fire alarm system." The federated
approach delivered comparable results while utilizing a less
computationally intensive model. In future work, we aim to
extend this approach to develop a real-world fire detection
system capable of maintaining robustness against sensor
failures.
\end{abstract}

\begin{IEEEkeywords}
machine learning (ML), algorithm, model, Artificial Neural
Network, Imputation, Sensor fusion, Fire Alarms, Fire
Detection, Error Handling, Federated Learning
\end{IEEEkeywords}

\section{Introduction and Motivation}\label{intro} 

Fire alarm systems play a critical role in safeguarding
lives and property by providing early warnings of fire
hazards. Traditional systems, however, do not perform as
well as their critical role would suggest. For example, a
2013 study found the Honeywell FS90, a widely used
commercial fire detection system, only achieves an accuracy
of 87.5\%\cite{smokeacc}. This contributes to deaths,
injuries, and damage due to fires in two ways. (1) When a
fire occurs and the fire detection system fails to recognize
it, the occupants of the building can only react to the fire
once they are close enough to observe it, often once it is
too late to evacuate. (2) A 1995 report stated that fewer
than 25\% of people interpreted the sound of the fire alarm
as a potential indication of a real emergency\cite{crywolf}.
Due to the high frequency of false or 'nuisance' alarms,
occupants may not evacuate or contact emergency services,
even if the alarm is triggered by a real fire.

As these systems typically rely on just one or two sensors
and simple logic to detect fires, they are especially
susceptible to faulty sensors rendering them completely
ineffective. While artificial intelligence (AI)-based fire
detectors have demonstrated significant improvements in
accuracy over traditional systems, they generally rely on
error-free sensor data\cite{ai1}\cite{ai2}\cite{ai3}. This
limits their practical applicability, especially in
real-world scenarios wherein sensor errors and failures are
common.

To address these challenges, we propose AlarmNet, a robust,
AI-based fire detection system specifically engineered to
remain accurate in the presence of sensor errors. AlarmNet
is trained on a dataset from which randomly selected
measurements were removed to simulate missing or faulty
sensor readings. This approach enables the model to learn to
detect fires accurately based on unreliable sensor data. 

The system is evaluated in two configurations: a centralized
model trained on a single server and a simulation of a
federated model, where training is distributed across
multiple clients and aggregated into a global model. This
dual approach allows us to investigate two key questions:
(1) whether it is possible to train a model that can handle
sensor errors, and (2) whether a federated model can achieve
comparable performance while utilizing fewer computational
resources. If the federated model meets this goal, it would
demonstrate AlarmNet's viability as a real-world fire
detection system. This paper first details the analysis and
preparation of the chosen dataset, followed by a disussion
of the design, implementation, and evaluation of both
versions of AlarmNet.

\section{Approach}

\section{Dataset and Training Setup}
\section{Dataset}
The dataset used in this project is the Smoke Detection
Dataset provided by Stefan Blattmann\cite{dataset}. The
dataset consists of approximately 62,629 sensor readings
encompassing both normal and fire-related scenarios,
providing diverse training and evaluating fire detection
models.

To generate this data, Blattman constructed an
Arduiono-based system to collect data from an array of six
sensors at a 1Hz sampling rate. These sensors were:
\begin{itemize}
    \item Bosch BMP390: Pressure Sensor
    \item Bosch BME688: Humidity, Temperature, and Gas
    Sensor (Volatile Organic Compounds/VOC)
    \item Bosch BMP388: Pressure Sensor
    \item Sensirion SPS30: Particular Matter Sensor
    \item Sensirion SHT31: Humidity and Temperature Sensor
    \item Sensirion SPG30: Gas sensor (VOC)
\end{itemize}
Note that several sensors measure the same environmental
factors, such as humidity and pressure. This redundancy is
intentional and serves to enhance the robustness of the
model. Before uploading this dataset to Kaggle, Blattman
utilized sensor fusion to combine the data from these six
sensors into a single dataset. The only sensor with no
redundancy is the Sensirion SPS30, which measures
particulate matter and is the most expensive sensor.

Blattman placed his sensor array into various fire scenarios and recorded the measurements. He provided the following short list of scenarios during which he collected data:
\begin{itemize}
    \item Normal indoor
    \item Normal outdoor
    \item Indoor wood fire, firefighter training area
    \item Indoor gas fire, firefighter training area
    \item Outdoor wood, coal, and gas grill
    \item Outdoor high humidity
    \item etc.
\end{itemize}
\cite{Blattmann}

\subsection{Feature Elimination}
The dataset provided by Blattman originally included 15 features, which are listed in Table \ref{tab:features}. Of these 15, the two sample count features and the timestamp were removed as they do not provide any useful information for the model. Of the remaining 12 features, a crucial task was to determine which features were the most relevant to the target variable. Eliminating irrelevant variables reduces the complexity and computational cost of the model, as its input space shrinks in dimensionality. This reduction can also help prevent overfitting, as the model is less likely to learn noise in the data.

\begin{table}[ht]
    \begingroup
    \renewcommand{\arraystretch}{1.5} % 
    \begin{tabular}{|p{0.25\linewidth}|p{0.65\linewidth}|}
        \hline
        \textbf{Feature} & \textbf{Description} \\
        \hline
        Timestamp & UTC Timestamp of the sample \\
        \hline
        Number & Unique identifier for each sample \\
        \hline
        Unnamed: 0 & Unintended duplicate of Number \\
        \hline
        Temperature & Air temperature in degrees Celsius \\
        \hline
        Humidity & Air humidity in percentage \\
        \hline
        Pressure & Air pressure in hectoPascals \\
        \hline
        TVOC & Total Volatile Organic Compounds in parts per billion \\
        \hline
        eC02 & Carbon Dioxide in parts per million \\
        \hline
        Raw H2 & Raw, uncompensated molecular hydrogen \\
        \hline
        Raw Ethanol & Raw, uncompensated ethanol \\
        \hline
        PM1.0 & Percentage of particles in the air less than 1.0$\mu m$ in diameter \\
        \hline
        PM2.5 & Percentage of particles in the air less than 2.5$\mu m$ in diameter \\
        \hline
        NC0.5 & Number concentration of particles in the air greater than 0.5$\mu m$ in diameter \\
        \hline
        NC1.0 & Number concentration of particles in the air greater than 1.0$\mu m$ in diameter \\
        \hline
        NC2.5 & Number concentration of particles in the air greater than 2.5$\mu m$ in diameter \\
        \hline
    \end{tabular}
    \vspace{1pt}
    \caption{Smoke Detection Dataset Features}
    \label{tab:features}
    \endgroup
\end{table}

To achieve this, we calculated the absolute correlation of
each feature with the target variable, 'Fire Alarm'. The
features were then ranked based on this correlation, and the
top four features were selected for the model. This number
was chosen by incrementally removing the least correlated
features from the dataset and training a model on the these
data until a significant drop in accuracy was measured.
These top four features were:
\begin{enumerate}
    \item Humidity
    \item Raw Ethanol
    \item Pressure
    \item TVOC
\end{enumerate}
While a sufficiently accurate model could be trained on these data, this list of features does not include any of the particulate matter measurements. If this system were to ignore one of its sensors, it would be significantly less resilient against sensor errors. To address this, we added back the NC0.5 feature to the data, as it is the most correlated with the target variable among the particulate matter measurements.

\subsection{Error Insertion}
To simulate sensor errors, we developed a method to randomly remove certain measurements from the dataset according to some assumed rate of error. This rate of error represents the chance that a given sensor reading will provide incorrect or missing data during any given sample. We chose to assume this error rate to be 40\%. This is an unrealistically high number, but it is low enough to ensure that the model is not completely useless while still providing a significant challenge. 

Most of the features--with the exception of the particulate matter measurements--were collected using two sensors. To include this redundancy in the simulation, we assumed that both sensors that generate any feature would have to fail simultaneously in order for that feature to be missing for a given a sample. In other words, while 40\% of the particulate matter measurements were removed, only 20\%\footnote{The joint probability formula $P(A\cap B=P(A)\times P(B))$ implies that this probability should actually be 16\%. We chose to use 20\% instead for the added challenge.} of the other measurements were removed. After applying this error simulation to the dataset, approximately 24.2\% of the data was removed.

\subsection{Imputation}
Before this data could be used to train a model, the missing values had to be replaced. We chose to use median imputation, which replaces missing values with the median of the corresponding feature calculated from the training dataset. This method was chosen because it is robust to outliers and skewed distributions, which are common in sensor data.

\subsection{Model Architecture}
The model implemented a custom deep neural network
(AlarmNet) for fire detection. The dataset's multivariate
sensor data balance complexity and efficiency. Input layer
size depends on the number of features such as 12 features
as the initial full-feature model. It was reduced to 4
features via a correlation ranking system. Hidden layers
were added with (62,32) for a 12-feature model and a
5-feature model. The 4-feature system uses (64,64) to
provide additional capacity. All these models activated
functions with ReLU( Rectified Linear Unit) introducing
non-linearity and mitigating the vanishing gradient problem.
The training model algorithm implemented the Adam Optimizer
technique for its adaptive learning rate abilities with an
initial learning rate of 1e-3 for most models and 1e-5 for
the imputed error model. Parameters are updated using
gradient descent with momentum adjustment for faster
convergence. The learning rate is based on validation loss
improvements, configured with a factor of 0.5 to halve the
learning rate when performed. Having a patience of 4000
iterations without improvement and a threshold of 1e-3 for
significant improvements. In the training configuration,
3000 epoch models were extended to 16,000 for the imputed
error model to ensure convergence. Using epoch will improve
based on the device GPU accelerated training on CUDA when
available, otherwise CPU. 

\subsection{Evaluation Methods}
\subsubsection{Multiple Models}
To ensure that the techniques used in this project are effective, we trained and evaluated multiple models. We trained a model based on the full 12-features of the dataset, a model based on the top 4 features, a model based on the top 5 features, and a model that included the NC0.5 feature for redundancy. We also trained a model that included the imputed error data. The performance of each model was evaluated using the following metrics:
\subsubsection{Metrics}
The performance of each model was evaluated using the following metrics:
\begin{itemize}
    \item Recall: Measures the proportion of actual fire
    cases detected by the model to minimize false negatives.
    This is the most important metric, as it is crucial that
    the model detects as many fires as possible.
    \begin{equation}
        \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
    \end{equation}
    \item Precision: The proportion of correctly identified fire alarms
    \begin{equation}
        \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
    \end{equation}
    \item F1-Score and Accuracy: Implements the harmonic mean of precision and recall with overall correctness of the model.
    \begin{equation}
        F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
    \end{equation}
    \item Confusion Matrix: A visualization tool that lists the true positive/negative and false positive/negatives
\end{itemize}



\begin{itemize}
\item Data Splitting: The dataset is split into 80/20
percent for training/testing to evaluate model performance. 
\item Hardware: A code was implemented to perform on
GPU(CUDA) for acceleration; otherwise CPU is available. 
\item Training Configuration: Adam optimizer was implemented
to configure with learning rates of 1e-3 for most models and
1e-5 for the imputed error model. Learning rate by a factor
of 0.5, the patience of 4000 epochs, and threshold 1e-3. In
most models, 3000 epochs were implemented while 16,000
epochs were for the imputed error model to ensure proper
convergence on data. 
\item Precision: Proportion of correctly identified fire
alarms 
\begin{equation}
    \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
\end{equation}
\item Recall: Measures the proportion of actual fire cases
detected by the model to minimize false negatives
\begin{equation}
    \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
\end{equation}
\item F1-Score and Accuracy: Implements harmonic mean of
precision and recall with overall correctness of the model.
\begin{equation}
    F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
\end{equation}
\item FPR: False Positive Rate measures the actual negatives
that are incorrectly classified as positives 
\begin{equation}
    \text{FPR} = \frac{\text{FP}}{\text{FP} + \text{TN}}
\end{equation}
\item Confusion Matrix: This model visualizes true
positive/negative and false positive/negatives
\end{itemize}
\subsubsection{Honeywell FS90 Simulation}



\subsection{Model Architecture and Training}

\textbf 
In the real world sensor data, is often due to hardware
communication limitations and errors. It can lead to
incomplete or noisy environmental factors. The reliability
of the model can go in such conditions in error handling
using robust strategies to incorporate into the model and
training process. These strategies allowed the model to
maintain high accuracy and recall during the sensor error
process. In simulating real-world scenarios, errors were
intentionally introduced into the dataset. In the model run,
20 percent was applied across all features, with particulate
matter (PM) sensors likely to encounter errors due to
insufficient redundancy. The dataset showed four sensors in
their categories, temperature and Humidity, Pressure, (TVOC,
CO2, Ethanol, H2), and Particulate Matter ( NC0.5, NC1.0,
NC2.5, PM1.0, PM2.5). erroneous values were represented as
missing data (NaN) or extreme outliers to emulate hardware
failures or any communication drops. Missing values were
handled using median imputation, which replaces erroneous
values with the median of the corresponding feature
calculated from the training dataset. The median was chosen
against outliers suitable for skew distributions in sensor
data. Imputation implementation ensures the dataset remains
intact, allowing it to learn effectively. The feature was
further improved by identifying the most correlated features
with fire detection. Key features such as TVOC, Pressure,
Raw Ethanol, and Humidity were retained.    
The training process specialized adjustments for handling
noisy and incomplete data. During the deep ANN error
handling architecture of [(256,256,256)] for better
resolution. Extending training epochs were used, enabling
the model to generalize effectively. In the error handling
stage, errors were introduced to replicate real-world
conditions challenging the robustness of the architecture.
The implementation strategy introduced erroneous or missing
values were replaced with the median of each feature to
maintain data integrity. The imputation approach was
particularly effective for skewed feature distributions
which enchanted the model ([256,256,256]) and demonstrated
resilience to noisy and incomplete data. The imputed error
model showcased its ability to handle a 20 percent sensor
error rate with minimal performance degradation and
showcasing minimized false negatives under challenging
conditions. Performance metrics were involved in all models
to evaluate using recall, precision, F1-Score, and confusion
matrices. The model was benchmarked against the Honeywell
FS90 fire detection system. Comparison revealed superior
recall and resilience in the proposed model, under simulated
noisy conditions, such as histograms of missing values and
bar charts comparing recall and recall without errors
showing the model's robustness. Furthermore, the size and
fast inference of the final model exported in ONNX format
ensure the deployment in real-time IoT scenarios of fire
detection systems. These error-handling strategies
demonstrate the model's ability to adapt to real-world
challenges for a reliable solution in fire detection in
different environments. Simulating error handling,
imputation, and targeted features in correlation highlights
the model's resilience and practical applications. 

\section{Results and Analysis}
In a comparative analysis, SVM(Support Vector Machine) and
NN (Neural Network) were both considered for fire detection
within the 12-feature test bench. Neural networks are
ultimately chosen for their superior flexibility and
robustness. SVMs are known for their simplicity and
effectiveness within smaller datasets due to their ability
to model linear and non-linear relationships using kernel
functions. However, in this dataset, the project had a large
size of 62,000 samples which limited the SVM training model
as memory requirement scale quadratically with the number of
samples. On the other hand neural network demonstrates
superior performance and adaptability. They efficiently
scaled with large datasets by leveraging batch size gradient
descent and GPU acceleration with mixed precision and
gradient scaling. It can learn hierarchical features
representing them as particularity effective for
multi-sensor data its architecture model is tailored to
handle noisy and missing data, as seen in the imputed error
model under results., It can maintain high recall and
precision under a 20 percent error rate. The full feature
model utilizes all 12 -features that achieved high precision
and recall but its computational complexity made it less
efficient. After using a correlation technique ranking them
by target variable it reduced to 4 and 5 feature sets
retaining only the top four features ( Humidity, Raw
Ethanol, Pressure, and TVOC), achieved performance to the
full feature model with minimal precision loss of 0.1
percent. It significantly improved making it more suitable
for different environments. The redundant feature model
added the NC0.5 enchanting redundancy and improved
robustness maintaining efficiency. The imputed error model
designed to handle noisy and missing data shows a resilience
with 20 percent error rate. Thus outperforming the Honeywell
FS90 system. In addition to accuracy and recall, time was
evaluated to determine real-time applicability. The proposed
models achieve an average time of 2e-2 seconds per sample
making them viable for deployment in fire detection systems.
Evaluation metrics showed visual comparison including
confusion matrices, and bar charts illustrating the superior
performance. The reduced features and imputed error and
aggregation demonstrated the ability to balance accuracy,
efficiency, and robustness for real-world fire detection
applications. The Federated model trains the local copy of a
global model using local data. The training sends model
weights (weights of the neurons in the neural network) to
the global mode. The federate and AlarmNet achieve near
identical accuracy. These bar graphs show the comparison of
recall with and without error and precision with 20 percent
error and without. With Machine Learning algorithms, it was
able to provide excellent validation against Honeywell FS90
highlighting the potential ANN that can provide for
enchanting better real-world safety protocols (fig 18 and
19). 

\section{Lessons Learned}
This machine learning model successfully demonstrated the
development an efficient fire detection system using sensor
data. By leveraging a custom neural network architecture,
the system achieved high precision and recall even with
real-world scenarios such as noisy or missing sensor data.
Feature selections and target preparation, including median
imputation and error handling, ensure the model's
exploration. The reduced feature model maintained comparable
performance to the 12-feature model significantly reducing
computational requirements and making it good for
constrained environments. Comparative analysis highlights
the proposed neural network over the traditional such as
SVM. The neural network can handle large datasets, capture
complex features tolerate sensor errors underscores the
effects in IoT-base fire detection systems. Optimizing time
and compact model size ensure viability for read-time
devices. The project object results emphasize the importance
of designing models that are balanced, efficient, accurate,
and robust for safety and critical protocol applications.
Future work can be explored further optimizing the learning,
advancing imputation, and improving the federated model
weights between three clients to enhance system performance.
The project demonstrates the potential for machine learning
to revolutionize fire detection technology to challenge
high-end companies such as Honeywell to ensure the power of
ANN to contribute to improving safety and reliability in
diverse environments.






\section{Contributions}
\begin{itemize}
\item\textbf{Matthew Anderson:} 
\begin{itemize}
    \item Federated Learning Class
    \item Edited Report and Presentation
\end{itemize}

\item\textbf{Jaskin Kabir:} 
\begin{itemize}
    \item Project Manager
    \item Dataset Selection, Preparation, and Analysis
    \item AlarmNet Class that contained the fully connected
    model
    \item Error insertion and imputation
    \item Edited Report and Presentation
\end{itemize}

\item\textbf{Axel Leon Vasquez:} 
\begin{itemize}
    \item Wrote Report
    \item Created Presentation
    \item Data Visualization
\end{itemize}
\end{itemize}
\subsection{References}

\bibliographystyle{IEEEtran}
\bibliography{refs.bib}



\subsection{Results} 

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{images/SVM.png}
    \caption{SVM Metric}
    \label{fig: SVM Model}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{images/12acc.png}
    \caption{12 - Feature Metrics}
    \label{fig: 1.0 }
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{images/12CM.png}
    \caption{12 - Feature Loss Curve}
    \label{fig: 1.2}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{images/12CMM.png}
    \caption{12 - Feature Confusion Matrix}
    \label{fig:1.3}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{images/Corr.png}
    \caption{Features}
    \label{fig:2.0-label}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{images/4metric.png}
    \caption{4 - Feature Metrics}
    \label{fig:3.0}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{images/4LC.png}
    \caption{4 - Feature Loss Curves}
    \label{fig:3.1}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{images/4CM.png}
    \caption{4 - Feature Confusion Matrix}
    \label{fig:3.2}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{images/5metric.png}
    \caption{5 - Feature Metrics}
    \label{fig:4.0}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{images/5LC.png}
    \caption{5 - Feature Loss Curves}
    \label{fig:4.1}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{images/5CM.png}
    \caption{5 - Feature Confusion Matrix}
    \label{fig:4.2}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{images/imputationmetric.png}
    \caption{Imputation Metrics}
    \label{fig:5.0}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{images/ImputationLC.png}
    \caption{Imputation Loss Curves}
    \label{fig:5.1}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{images/ImputationCM.png}
    \caption{Imputation Confusion Matrix}
    \label{fig:5.2}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/12Fed.png}
    \caption{Federarted Learning: 12-Features}
    \label{fig:6.0}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/5Fed.png}
    \caption{Federated Learning: 5-Features}
    \label{fig:6.1}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/FedHandling.png}
    \caption{Federated Learning: Error handling}
    \label{fig:6.2}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{images/Recall.png}
    \caption{Recall Comparison Honeywell FS90 Vs AlarmNet ANN Vs Federated}
    \label{fig:7.0}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{images/PrecisionComparison.png}
    \caption{Precison Comparison Honeywell FS90 Vs AlarmNet ANN Vs Federated}
    \label{fig:7.1}
\end{figure}

\end{document}